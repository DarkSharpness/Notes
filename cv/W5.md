# W5

## Edge Detection

How to define a jump? (consider 1-demensional signal)

- Compare the similarity of target distribution with a pre-defined "jump" distribution.
- e.g. Defines jumps = 1, choose a point, where the similarity is the lowest.

Math form:

- use L0-norm to regulate the count of jumps (L0-norm counts the number of non-zero elements)
- with respect to:

$$
E(k) = \min _{f} \| f - g\|_2^2 + \lambda \|\nabla f - k\|_0
$$

Trivially, $E(k)$ is a function of $k$, and it is non-increasing. Empirically, we should find the point, where $E(k)$ decreases most sharply. Sadly, this reduce to the problem of finding a "jump" in the 1-demensional signal.

In real-world, we want to ensure that $k$ should not be too big (in other words, overfitting). Thus, we can add a regularization term to the objective function. We just simply remove $k$ from the objective function.

$$
\min _{f} \| f - g\|_2^2 + \lambda \|\nabla f \|_0
$$

In the end, this problem reduces to finding the best $\lambda$ that conforms to the human perception.

## Another approach

We may add another constraint:

$$
\min_{f,v} \|f - g\|_2^2 + \lambda \|\nabla v\|_0 + \beta \|v - \nabla f\|_2^2
$$

We may solve this problem by iterating the following two steps:

### Step 1. Fix $v$, solve $f$

$$
\min_{f} \|f - g\|_2^2 + \beta \|v - \nabla f\|_2^2 \\
$$

We have the closed-form solution:

$$
f = (I + \beta \nabla^T \nabla)^{-1} (g + \beta \nabla^T v)
$$

### Step 2. Fix $f$, solve $v$

$$
\min_{v} \lambda \|\nabla v\|_0 + \beta \|v - \nabla f\|_2^2
$$

Just consider the problem element-wise, we have:

$$
v_i = \begin{cases}
\nabla f_i & \text{if} \|\nabla f_i\|_2^2 > \frac{\lambda}{\beta} \\
0 & \text{otherwise}
\end{cases}
$$

In close form:

$$
v_i = \nabla f_i \cdot u(\nabla f_i, \sqrt{\frac{\lambda}{\beta}})
$$

where $u(x, y)$ is only $0$ around $x$, covering $[-y, y]$ and $1$ otherwise.

### Some details

Initialize $\beta$ with a small value (if too large, the $\beta\|\nabla f\|_2^2$ term will dominate the objective function). So we ought to increase $\beta$ gradually.
